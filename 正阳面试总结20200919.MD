# 正阳面试总结20200919
## <font color=blue>01、近期从事的项目，讲下项目介绍以及用到的组件情况。</font>
目前主要做的是农业银行天津分行的数据平台建设，该项目用到的技术主要是Hive、Impala、Kudu等。
## <font color=blue>02、该项目可以解决的问题。 </font>
目前关于银行报表的加工，主要基于原来的Sybase IQ数据库来做，随着业务量的变化，越来越难以适应目前的需求。经过了解，目前采取的办法就是将原本由IQ数据库进行的数据加工处理，迁移至Kudu技术平台上，增强目前数据加工处理能力。
## <font color=blue>03、该项目的数据来源。</font>
总行每日下发的数据文件。
## <font color=blue>04、该项目的总体处理流程。</font>
先获取数据文件，然后对其解压，清洗，视需求脱敏，入库。
## <font color=blue>05、数据出错，需要重跑，有没有预案，怎么确定数据异常。</font>
目前没有预案，主要通过观察结果表数据，如有异常会给与反馈。
## <font color=blue>06、之前的项目有没有参与过架构方面的工作中。</font>
参与过一些，主要是在天津国瑞数码工作期间，对数据迁移，数据备份，数据同步等工作。
同时在流式计算方面，针对HBASE中的ROWKEY设计，KAFKA中OFFSET维护，数据落地存储等方面参与过一些工作。
## <font color=blue>07、对拉链表怎么处理，拉链表的定义。</font>
拉链表是针对数据仓库设计中表存储数据的方式而定义的，顾名思义，所谓拉链，就是记录历史。记录一个事物从开始，一直到当前状态的所有变化的信息。\
首先我们要确定拉链表的时间粒度，比如说拉链表每天只取一个状态，也就是说如果一天有3个状态变更，我们只取最后一个状态，通过etl工具对操作型数据库按照时间字段增量抽取到ods或者数据仓库(每天抽取前一天的数据)，形成每天的增量数据\
[漫谈数据仓库之拉链表（原理、设计以及在Hive中的实现）](https://www.cnblogs.com/lxbmaomao/p/9821128.html)
## <font color=blue>08、流式项目的介绍，该项目的架构，用到的技术，自己做过的工作。</font>
在上一家公司，做过一个项目《互联网涉稳动态监测系统》，这个项目是对接山东省地方政府，对其地域内的网络舆情新闻进行动态监控。\
该项目用到的技术主要是JSOUP、ES、HBASE、KAFKA、SPARK、MYSQL等\
该项目数据源采取的爬虫爬取数据，将数据存储到本地，并且放到KAFKA中，由SPARK消费该数据，采用SPARK CEP对敏感内容进行预警，并且统计汇总舆情新闻的实时动态\
我在其中主要负责爬虫任务的定制化开发，以及SPARK代码的功能维护及需求更改。
## <font color=blue>09、反爬虫技术。</font>
1.1 通过Headers反爬\
1.2 基于用户行为反爬虫\
1.3 动态页面的反爬虫\
1.4 Cookie限制（登陆限制）\
1.5 验证码限制\
[常见反爬虫策略 及应对措施](https://blog.csdn.net/cui_yonghua/article/details/103787523)
## <font color=blue>10、数据存储对HBASE及ES选择考虑。</font>
从基本功能来说这两个确实有相似性，但是根据业务需求不同，我觉得有几点可以考虑：1. 查询复杂度：HBase支持简单的行或者range查询，比如给一个PK查该行的数据，或者给一个begin/end查这个范围的数据，如果想完成更复杂的功能就不太容易。而ES支持的查询比较丰富，或者说这些查询都带有一点复杂计算的味道了。比如你有个论坛，你想查帖子里面是否包含敏感词，如果采用HBase就比较麻烦，使用HBase你可以将帖子存进来、读出去，但是要查内容里面的东西，只能一点点过滤；而ES是可以比较方便的帮助你完成这个功能的；2. 数据量：按道理说两者都是支持海量数据的，但是据我个人感觉，HBase可能更容易支持更多的数据，因为其一开始设计就是解决海量问题的；而ES是后来慢慢增强其存储扩展性的；那么也就是说，HBase上手起来扩展性不太会阻碍你使用；ES可能要多费点劲。当然，听说也有人写了ES基于Azure或者S3的存储插件，但是稳定性不知道如何；3. 剩下的就是比较远的考虑，比如维护性，HBase基于Hadoop那一套，组件多，维护起来代价也不低，而ES自成体系，维护起来稍微好点；当然这个是相对的，绝对来说都不会容易。比如新功能开发，比如成本控制等等。。。\
[MySQL、HBase、ES的特点和区别](https://www.cnblogs.com/029zz010buct/p/10366775.html)
## <font color=blue>11、数据存储的数据格式，字段类型。</font>
数据存储是采用JSON格式，字段有ID地址、省、市、县、运营商、HTML源码、HTML去标签文本、页面爬取层数、域名、页面标题、URL地址、地域标识
## <font color=blue>12、项目存储数据量。</font>
爬取数据量大概在1.2亿条数据
## <font color=blue>13、爬虫爬取策略。</font>
每日定时爬取，然后会将爬取到的页面，与当前存储的页面的文本进行对比，采用HASH值对比。如果页面无变化，自动过滤，如果页面更改，则将此页面存下来。
## <font color=blue>14、项目的QPS。</font>
自己没亲子测过，但是据了解大概在七八百。
## <font color=blue>15、大数据集群规模。</font>
当时集群规模大概在15个节点，后续计划新增60个节点，但因疫情原因有所推迟。
## <font color=blue>16、跟行方沟通新技术的推行，解决的业务痛点。</font>
目前行方正在推进数据平台的建设，造成数据落地存储存在多个方案，需要一个数据同步方案。目前行方的意见是采用shell脚本加调度平台的方式实现数据同步。而这样的方案缺点就是在多数据源的情况下，拓展性极差。所以提出了两种解决方案，即DATAX与FLINKX。
## <font color=blue>17、SPARK处理的数据，是如何跟KAFKA对接的。</font>
spark-streaming对接kafka有两种方式：\
1.基于receiver的方式，属于高级API，简单但是效率低，容易丢失数据（可以设置WAL）。
它的原理是：receiver从kafka拉取数据存储到executor的内存中，spark-streaming启动job处理数据。偏移量保存在zookeeper中。 \
2.基于Direct的方式\
属于低级API，效率高。\
executor直接对接kafka的partition,触发action后，周期性地读取topic分区指定offset的数据，生成batch运算。\
优点：\
简言之，省去了receiver，降低资源消耗，提高性能。\
1.简化并行读写：Spark会创建和kafka partition数量一致的RDD partition,并且并行化从kafka中读取数据，所以在kafka的partition和rdd的partition之间，一一对映。\
2.高性能：direct没有receiver,不需要WAL写前日志，因为kafka中的副本就可以保证数据不丢失。\
3.降低资源，direct不需要receiver,因此申请的executor可以全部用于运算。\
缺点:\
1.开发复杂\
2.要spark自己维护offset\
[sparkStreaming 连接kafka的方式](https://www.jianshu.com/p/96727dad17c2)
## <font color=blue>18、KAFKA中的OFFSET如何维护。</font>
KAFKA自带维护OFFSET的能力（其由ZooKeeper或Broker来维护），但是为了保证数据的准确投递，可以采用手动维护方式，提供几种维护的方式，即采用数据库来维护。（比如MYSQL、HBASE、ES、MongoDB等）方式来维护
## <font color=blue>19、KAFKA如何保证数据消费的一致性。</font>
引入了 High Water Mark 机制，可以保证Consumer在读取所有Broker之间的信息都是一致的，可以通过参数replica.lag.time.max.ms参数配置，它指定了副本在复制消息时可被允许的最大延迟时间\
[Kafka 是如何保证数据可靠性和一致性](https://www.cnblogs.com/zz-ksw/p/12603216.html)
## <font color=blue>20、对MYSQL维护OFFSET如何考虑选型。</font>
在初始选型的时候，因为MYSQL的操作易用性，且预估我们的数据量不会很大，所以选用了MYSQL作为首选的维护方式。
## <font color=blue>21、OFFSET更新维护的逻辑。</font>
首先创建对应的数据库表，并初始化参数，然后在读取KAFKA数据时，首先先去MYSQL查询当前的OFFSET值，通过指定OFFSET来消费数据，消费完后，将新的OFFSET值更新入MYSQL。后面循环这个操作。
## <font color=blue>22、HBASE维护OFFSET可能会遇到什么问题。</font>
目前使用的方案已经比较良好的适用目前的业务场景，暂时没有考虑其他方案及可能存在的隐患。
## <font color=blue>23、SPARK中的RDD与Partition。</font>
Spark RDD 是一种分布式的数据集，由于数据量很大，因此要它被切分并存储在各个结点的分区当中。
Partition是分区，当我们对RDD进行操作时，实际上是对每个分区中的数据并行操作。\
[Spark 分区(Partition)的认识、理解和应用法](https://www.jianshu.com/p/3e79db80c43c?from=timeline&isappinstalled=0)
## <font color=blue>24、SPARK编程的模型大概是什么。</font>
Spark应用程序可分两部分：Driver部分和Executor部分\
Driver部分主要是对SparkContext进行配置、初始化以及关闭。\
Spark应用程序的Executor部分是对数据的处理\
[spark学习之-----spark编程模型](https://blog.csdn.net/chenxun_2010/article/details/79208951)
## <font color=blue>25、SPARK在与数据库交互的时候，如何减少连接数。</font>
从单个元素获取连接改为单个分区获取连接，如使用foreachPartition代替foreach，在foreachPartition内获取数据库的连接。\
[Spark高频面试题总结](https://blog.csdn.net/AiBigData/article/details/102824906)
## <font color=blue>26、SPARK中Partition的数量如何确定的。</font>
如果是读取hdfs的文件，一般来说，partition的数量等于文件的数量。\
如果是接入的KAFKA流式数据，partition的数量等于KAFKA分区数。\
[Spark：任务中如何确定spark分区数、task数目、core个数、worker节点个数、excutor数量](https://blog.csdn.net/weixin_38750084/article/details/82725176)
## <font color=blue>27、SPARK中的广播变量是做什么的。</font>
广播变量是一个只读变量，通过它我们可以将一些共享数据集或者大变量缓存在Spark集群中的各个机器上而不用每个task都需要copy一个副本，后续计算可以重复使用，减少了数据传输时网络带宽的使用，提高效率。相比于Hadoop的分布式缓存，广播的内容可以跨作业共享。\
广播变量要求广播的数据不可变、不能太大但也不能太小（一般几十M以上）、可被序列化和反序列化、并且必须在driver端声明广播变量，适用于广播多个stage公用的数据，存储级别目前是MEMORY_AND_DISK。\
[Spark中广播变量详解](https://zhuanlan.zhihu.com/p/158877265)
## <font color=blue>28、SPARK用的算子都有哪些，会产生shuffle的都有哪些。</font>
1、repartition类的操作：比如repartition、repartitionAndSortWithinPartitions、coalesce等\
2、byKey类的操作：比如reduceByKey、groupByKey、sortByKey等\
3、join类的操作：比如join、cogroup等\
[Spark会产生shuffle的算子](https://blog.csdn.net/sillyzhangye/article/details/86181343)
## <font color=blue>29、map与flatMap的区别。都适用于哪些场景。</font>
map：对 RDD 每个元素转换，文件中的每一行数据返回一个数组对象 \
flatMap：对 RDD 每个元素转换，然后再扁平化,将所有的对象合并为一个对象，文件中的所有行数据仅返回一个数组 对象，会抛弃值为 null 的值 
## <font color=blue>30、SPARK任务运行的话都需要做哪些配置。</font>
用得较多的参数是： 
--class:应用程序的入口点（例如，org.apache.spark.examples.SparkPi）\
--master：集群的master URL（例如，spark://localhost:7077）\
--deploy-mode:将driver部署到worker节点（cluster模式）或者作为外部客户端部署到本地（client模式），默认情况下是client模式\
--conf：用key=value格式强制指定Spark配置属性，用引号括起来\
--application-jar：包含应用程序和所有依赖的jar包的路径，路径必须是在集群中是全局可见的，例如，hdfs://路径或者file://路径\
--application-arguments：传递给主类中main函数的参数\
[Spark Submit提交应用程序](https://blog.csdn.net/l1028386804/article/details/80739399)
## <font color=blue>31、ReduceByKey与GroupByKey有哪些区别。</font>
reduceByKey：按照key进行聚合，在shuffle之前有combine（预聚合）操作，返回结果是RDD[k,v]\
groupByKey：按照key进行分组，直接进行shuffle\
虽然两个函数都能得出正确的结果， 但reduceByKey函数更适合使用在大数据集上。 这是因为Spark知道它可以在每个分区移动数据之前将输出数据与一个共用的key结合。\
建议使用reduceByKey。但是需要注意是否会影响业务逻辑\
[reduceByKey和groupByKey区别与用法](https://blog.csdn.net/weixin_41804049/article/details/80373741)
## <font color=blue>32、FLINK都有哪几种时间的形式，他们有什么区别。</font>
对于流式数据处理，最大的特点就是数据具有时间的属性，Flink根据时间的产生位置分为三种类型，事件生成时间（Event Time）、事件接入时间（Ingestion Time）、事件处理时间(Processing Time)。用户可以根据具体业务灵活选择时间类型。\
[Flink中的时间类型](https://www.cnblogs.com/ilovezihan/archive/2020/02/05/12262245.html)
## <font color=blue>33、FLINK都有哪几种窗口。</font>
窗口的生命周期，就是创建和销毁。\
窗口创建：当第一个元素进入到窗口开始时间的时候，这个窗口就被创建了。\
窗口销毁：当时间（ProcessTime、EventTime或者 IngestionTime）越过了窗口的结束时间，再加上用户自定义的窗口延迟时间（allowed lateness），窗口就会被销毁。\
Window 可以分成两类：\
CountWindow：按照指定的数据条数生成一个 Window，与时间无关。\
TimeWindow：按照时间生成 Window。\
对于 TimeWindow，可以根据窗口实现原理的不同分成三类：滚动窗口（Tumbling
Window）、滑动窗口（Sliding Window）和会话窗口（Session Window）。\
[详解 Flink 中的窗口(Window)](https://blog.csdn.net/BeiisBei/article/details/106031985?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight)
## <font color=blue>34、FLINK如何解决数据乱序。</font>
在使用EventTime处理Stream数据的时候会遇到数据乱序的问题，流处理从Event（事件）产生，流经Source，再到Operator，这中间需要一定的时间。虽然大部分情况下，传输到Operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络延迟等原因而导致乱序的产生，特别是使用Kafka的时候，多个分区之间的数据无法保证有序。因此，在进行Window计算的时候，不能无限期地等下去，必须要有个机制来保证在特定的时间后，必须触发Window进行计算，这个特别的机制就是Watermark。Watermark是用于处理乱序事件的。\
[Flink如何处理乱序数据？](https://blog.csdn.net/epubit17/article/details/102775009)
## <font color=blue>35、基于MYSQL如何做实时的数据同步（数据采集）。</font>
实时Binlog采集 + 离线处理Binlog还原业务数据这样一套解决方案\
[美团 MySQL 数据实时同步到 Hive 的架构与实践](https://blog.csdn.net/yyoc97/article/details/102365204)
## <font color=blue>36、HBASE中ROWKEY的设计。</font>
1.Rowkey的唯一原则\
2.Rowkey的排序原则\
3.Rowkey的散列原则\
4.Rowkey的长度原则\
[HBase之Rowkey设计总结](https://blog.csdn.net/u012834750/article/details/81708669)
## <font color=blue>37、如何解决热点问题。</font>
1.Reverse反转\
2.Salt加盐\
3.Hash散列或者Mod\
[HBase之Rowkey设计总结](https://blog.csdn.net/u012834750/article/details/81708669)
## <font color=blue>38、HBASE的预分区。</font>
<font color=red>关于ROWKEY设计，热点问题，预分区可以参考下面的视频\
开课吧-搞定智慧出行业务下的离线与实时项目-离线功能实战-SparkSQL的数据分析-1:04:00</font>\
HBase默认建表时有一个region，这个region的rowkey是没有边界的，即没有startkey和endkey，在数据写入时，所有数据都会写入这个默认的region，随着数据量的不断  增加，此region已经不能承受不断增长的数据量，会进行split，分成2个region。在此过程中，会产生两个问题：1.数据往一个region上写,会有写热点问题。2.region split会消耗宝贵的集群I/O资源。基于此我们可以控制在建表的时候，创建多个空region，并确定每个region的起始和终止rowky，这样只要我们的rowkey设计能均匀的命中各个region，就不会存在写热点问题。自然split的几率也会大大降低。当然随着数据量的不断增长，该split的还是要进行split。像这样预先创建hbase表分区的方式，称之为预分区。\
[HBase学习之六: hbase的预分区设计](https://blog.csdn.net/javajxz008/article/details/51913471)

## <font color=blue>39、HBASE的查询是基于什么条件来做。</font>
实现条件查询功能使用的就是scan方式，scan在使用时有以下几点值得注意：\
1、scan可以通过setCaching与setBatch方法提高速度（以空间换时间）；\
2、scan可以通过setStartRow与setEndRow来限定范围。范围越小，性能越高。\
通过巧妙的RowKey设计使我们批量获取记录集合中的元素挨在一起（应该在同一个Region下），可以在遍历结果时获得很好的性能。\
3、scan可以通过setFilter方法添加过滤器，这也是分页、多条件查询的基础。\
[HBase条件查询（多条件查询）](https://blog.csdn.net/varyall/article/details/80321997)
## <font color=blue>40、CDH的搭建是基于什么样的方式。</font>
基于物理服务器或者容器搭建CDH\
[Docker容器部署CDH6.3.0](https://blog.csdn.net/qq_39680564/article/details/99478888)
## <font color=blue>41、CDH的高可用。</font>
[CDH高可用部署](https://blog.csdn.net/tony_328427685/article/details/86520329)\
[CDH cm节点高可用方案](https://blog.csdn.net/wulantian/article/details/46546003)
## <font color=blue>42、项目里用到过SpingCloud的哪些功能组件。</font>
服务发现——Netflix Eureka\
客服端负载均衡——Netflix Ribbon\
断路器——Netflix Hystrix\
服务网关——Netflix Zuul\
分布式配置——Spring Cloud Config\
[SpringCloud分布式开发五大组件详解](https://blog.csdn.net/weixin_40910372/article/details/89466955)\
[Springcloud常用组件功能及作用总结](https://blog.csdn.net/weixin_43676037/article/details/105350021)
## <font color=blue>43、SpingBoot常用的注解。</font>
@Controller、@RequestBody、@RequestMapping、@PathVariable、@RequestParam、@Repository、@Service、@Autowired、@Entity
## <font color=blue>44、Controller与RestController有什么区别。</font>
1.使用@Controller 注解，在对应的方法上，视图解析器可以解析return 的jsp,html页面，并且跳转到相应页面
若返回json等内容到页面，则需要加@ResponseBody注解\
2.@RestController注解，相当于@Controller+@ResponseBody两个注解的结合，返回json数据不需要在方法前面加@ResponseBody注解了，但使用@RestController这个注解，就不能返回jsp,html页面，视图解析器无法解析jsp,html页面
## <font color=blue>45、JAVA多线程用到过么。</font>
所谓多线程是指一个进程在执行过程中可以产生多个更小的程序单元，这些更小的单元称为线程，这些线程可以同时存在，同时运行，一个进程可能包含多个同时执行的线程。\
[Java多线程看这一篇就足够了](https://www.cnblogs.com/java1024/p/11950129.html)\
[Java中的多线程你只要看这一篇就够了](https://www.cnblogs.com/wxd0108/p/5479442.html)
## <font color=blue>46、NGINX在哪些场景中用到过。</font>
Nginx主要功能：1、反向代理 2、负载均衡 3、HTTP服务器（包含动静分离） 4、正向代理 \
[Nginx主要功能及使用](https://www.cnblogs.com/gucb/p/11990979.html)
## <font color=blue>47、VUE的生命周期。</font>
每个VUE实例在被创建之前都要经过一系列的初始化过程，这个过程就是VUE的生命周期。\
[详解vue生命周期](https://segmentfault.com/a/1190000011381906)
## <font color=blue>48、麒麟有接触么。</font>
没接触过
## <font color=blue>49、在离线的函数计算的时候，有一些条件不相等的怎么处理。</font>
<> 不等号 或 !=也行
## <font color=blue>50、讲一下UDF函数。</font>
UDF（User-Defined Functions）即是用户定义的hive函数。hive自带的函数并不能完全满足业务需求，这时就需要我们自定义函数了\
[hive之UDF函数编程详解](https://www.cnblogs.com/xuziyu/p/10754592.html)\
[Hive的内置函数和自定义函数UDF](https://blog.csdn.net/yu0_zhang0/article/details/79060849)