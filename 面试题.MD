# 面试题
## 1、相关名词
作用|作用|API
:-:|:-:|:-:
KAFKA|ACK|生产者
KAFKA|ISR|副本
KAFKA|AR|副本
KAFKA|HIGH WATER MARK|数据一致性
KAFKA|Receiver|高阶API
KAFKA|Direct|低阶API
HIVE|parquet|文件格式
HIVE|ORCFile|文件格式
HIVE|Snappy|压缩算法
HIVE|lzo|压缩算法
FLINK|WATER MARK|水印
ES|translog|日志文件
## HDFS写流程
1、客户端跟namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在，用户是否有权限等
2、namenode返回是否可以上传
3、client请求第一个 block该传输到哪些datanode服务器上
4、namenode返回3个datanode服务器ABC
5、client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将整个pipeline建立完成，逐级返回客户端
6、client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答
7、当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。
## 39.mapreduce的大致流程
答：主要分为八个步骤
1/对文件进行切片规划
2/启动相应数量的maptask进程
3/调用FileInputFormat中的RecordReader，读一行数据并封装为k1v1
4/调用自定义的map函数，并将k1v1传给map
5/收集map的输出，进行分区和排序
6/reduce task任务启动，并从map端拉取数据
7/reduce task调用自定义的reduce函数进行处理
8/调用outputformat的recordwriter将结果数据输出
## mapreduce的原理
答：mapreduce的原理就是将一个MapReduce框架由一个单独的master JobTracker和每个集群节点一个slave TaskTracker共同组成。master负责调度构成一个作业的所有任务，这些的slave上，master监控它们的执行，重新执行已经失败的任务。而slave仅负责执行由maste指派的任务。
## hadoop运行原理
答：hadoop的主要核心是由两部分组成，HDFS和mapreduce，首先HDFS的原理就是分布式的文件存储系统，将一个大的文件，分割成多个小的文件，进行存储在多台服务器上。
Mapreduce的原理就是使用JobTracker和TaskTracker来进行作业的执行。Map就是将任务展开，reduce是汇总处理后的结果。
## 2、HIVE执行步骤：
1.用户提交查询等任务给 Driver。
2.Compiler 编译器获得该用户的任务计划Plan。
3.Compiler 编译器根据用户任务去 MetaStore元数据库 中获取需要的Hive的元数据信息。
4.Compiler编译器  得到元数据信息，对任务进行编译，先将 HiveQL 转换为抽象语法树，然后将抽象语法树转换成查询块，将查询块转化为逻辑的查询计划，重写逻辑查询计划，将逻辑计划转化为物理的计划（MapReduce）, 最后选择最佳的策略。
5.将最终的计划提交给Driver。
6.Driver 将计划 Plan 转交给 ExecutionEngine执行引擎 去执行，获取元数据信息，提交给 JobTracker作业跟踪器 或者 SourceManager源管理器 执行该任务，任务会直接读取 HDFS 中文件进行相应的操作。
7.获取执行的结果。
8.取得并返回执行结果。
## 3、Hive 数据倾斜问题： 
倾斜原因： map 输出数据按 Key Hash 分配到 reduce 中,由于 key 分布不均匀、或者业务数据本身的特点。等原因造成的 reduce 上的数据量差异过大。 
1.1)key 分布不均匀 
1.2)业务数据本身的特性 
1.3)SQL 语句造成数据倾斜 
解决方案： 1>参数调节： 2>SQL 语句调节
## hive如何调优
答：hive最终都会转化为mapreduce的job来运行，要想hive调优，实际上就是mapreduce调优，可以有下面几个方面的调优。解决数据倾斜问题，减少job数量，设置合理的map和reduce个数，对小文件进行合并，优化时把握整体，单个task最优不如整体最优。按照一定规则分区。
## hbase的读操作：
ZooKeeper---meta--regionserver--region--memstore--storefile
1、首先从zookerper找到meta表的region的位置，然后读取meta表中的数据。而meta中又存储了用户表的region信息
2、根据namespace、表名和rowkey根据meta表中的数据找到写入数据对于的region信息
3、然后找到对于的regionserver
4、查找对应的region
5、先从Memstore找数据，如果没有，再到StoreFile上读
## 13.RDD机制？ 
答：rdd分布式弹性数据集，简单的理解成一种数据结构，是spark框架上的通用货币。 
所有算子都是基于rdd来执行的，不同的场景会有不同的rdd实现类，但是都可以进行互相转换。 
rdd执行过程中会形成dag图，然后形成lineage保证容错性等。 从物理的角度来看rdd存储的是block和node之间的映射。
## 15、spark工作机制？ 
答：用户在client端提交作业后，会由Driver运行main方法并创建spark context上下文。 
执行rdd算子，形成dag图输入dag scheduler，按照rdd之间的依赖关系划分stage输入task scheduler。 task scheduler会将stage划分为task set分发到各个节点的executor中执行。
## 16、spark的优化怎么做？ 
答： spark调优比较复杂，但是大体可以分为三个方面来进行，1）平台层面的调优：防止不必要的jar包分发，提高数据的本地性，选择高效的存储格式如parquet，2）应用程序层面的调优：过滤操作符的优化降低过多小任务，降低单条记录的资源开销，处理数据倾斜，复用RDD进行缓存，作业并行化执行等等，3）JVM层面的调优：设置合适的资源量，设置合理的JVM，启用高效的序列化方法如kyro，增大off head内存等等
## 14. 对于 Spark 中的数据倾斜问题你有什么好的方案？ 
1）前提是定位数据倾斜，是 OOM 了，还是任务执行缓慢，看日志，看 WebUI 
2)解决方法，有多个方面,避免不必要的 shuffle，如使用广播小表的方式，将 reduce-side-join 提升为map-side-join·分拆发生数据倾斜的记录，分成几个部分进行，然后合并 join 后的结果·改变并行度，可能并行度太少了，导致个别 task 数据压力大·两阶段聚合，先局部聚合，再全局聚合·自定义 paritioner，分散 key 的分布，使其更加均匀详细
## 20.什么是 shuffle，以及为什么需要 shuffle？ 
shuffle 中文翻译为洗牌，需要 shuffle 的原因是：某种具有共同特征的数据汇聚到一个计算节点上进行计算
## 2、PANDAS相关API
作用|作用|API
:-:|:-:|:-:
导入数据|pd.DataFrame(dict)|从字典对象导入数据，Key是列名，Value是数据
导入数据|pd.read_csv(filename)|从CSV文件导入数据
导入数据|pd.read_sql(query, connection_object)|从SQL表/库导入数据
导出数据|to_csv(filename)|导出数据到CSV文件
导出数据|df.to_sql(table_name, connection_object)|导出数据到SQL表
导出数据|df.to_json(filename)|以Json格式导出数据到文本文件
查看、检查数据|df.head(n)|查看DataFrame对象的前n行
查看、检查数据|df.shape|查看行数和列数
查看、检查数据|df.info()|查看索引、数据类型和内存信息
查看、检查数据|df.describe()|查看数值型列的汇总统计
数据选取|df[col]|根据列名，并以Series的形式返回列
数据选取|s.loc['index_one']|按索引选取数据
数据选取|df.query('[1, 2] not in c')| 返回c列中不包含1，2的其他数据集
数据清理|pd.isnull()|检查DataFrame对象中的空值，并返回数据清理|一个Boolean数组
数据清理|df.dropna()|删除所有包含空值的行
数据清理|df.fillna(x)|用x替换DataFrame对象中所有的空值
数据处理|df.sort_values(col1)|按照列col1排序数据，默数据处理，默认升序排列
数据处理|df.groupby(col1)|返回按列col1数据处理
数据合并|df1.append(df2)|将df2中的行添加到df1的尾部
数据合并|df.concat([df1, df2],axis=1)|将df2中的列添数据合并|加到df1的尾部
数据合并|df1.join(df2,on=col1,how='inner')|对df1的列和df2的列执行SQL形式的join
数据统计|df.describe()|查看数据值列的汇总统计
数据统计|df.mean()|返回所有列的均值
数据统计|df.corr()|返回列与列之间的相关系数
数据统计|df.count()|返回每一列中的非空值的个数
数据统计|df.max()|返回每一列的最大值
数据统计|df.min()|返回每一列的最小值
数据统计|df.median()|返回每一列的中位数
数据统计|df.std()|返回每一列的标准差
